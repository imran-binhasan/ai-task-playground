# AI Prompt Playground

A full-stack application for experimenting with AI language models, allowing users to adjust parameters like model selection and temperature to understand their effects on AI responses.

## âœ¨ Features

* ğŸ¤– **Multiple AI Model Selection** â€” Choose between different model tiers
* ğŸŒ¡ï¸ **Temperature Control (0-1)** â€” Adjust response creativity vs consistency
* ğŸ“œ **Conversation History** â€” Timeline of all prompts and responses with timestamps
* ğŸ¨ **Modern, Responsive UI** â€” Clean interface with Tailwind CSS
* ğŸ“ **Real-time Response Generation** â€” Instant feedback with loading states
* ğŸ”„ **Mock Mode** â€” Test without OpenAI API key using intelligent simulation
* ğŸ“š **API Documentation** â€” Interactive Swagger docs for backend
* âœ… **Form Validation** â€” Real-time input validation with helpful error messages
* ğŸ”” **Toast Notifications** â€” User-friendly success and error feedback

## ğŸ› ï¸ Tech Stack

**Frontend:**
* Next.js 16 (App Router)
* TypeScript
* Tailwind CSS
* Shadcn UI Components
* Axios for API requests

**Backend:**
* Node.js + Express.js
* OpenAI API Integration
* Express Validator for input validation
* Swagger/OpenAPI Documentation

## ğŸ“¦ Setup & Installation

### Prerequisites
- Node.js 18+ and npm
- OpenAI API key (optional â€” works in mock mode without it)

### Backend Setup
```bash
cd backend
npm install
cp .env.example .env

# Edit .env and configure:
# - OPENAI_API_KEY=your_key_here (optional)
# - USE_REAL_LLM=false (set to true to use real OpenAI)

npm run dev
# Server runs on http://localhost:5000
```

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
# App runs on http://localhost:3000
```

### Access Points
- **Application**: `http://localhost:3000`
- **API Documentation**: `http://localhost:5000/api-docs`
- **API Base**: `http://localhost:5000/api`

---

## ğŸ§  LLM Understanding

### What is an LLM?

A **Large Language Model (LLM)** is an artificial intelligence system trained on vast amounts of text data to understand and generate human-like language. LLMs learn patterns, grammar, facts, and reasoning capabilities from this training data.

**Key capabilities:**
- Answer questions and explain concepts
- Write creative content (stories, code, essays)
- Translate between languages
- Summarize long documents
- Solve problems through reasoning

**Popular examples:** GPT-4, Claude, Gemini, LLaMA

### What Does Temperature Mean?

Temperature is a parameter (0â€“1) that controls the **randomness** of the model's responses:

| Temperature | Behavior | Best For |
|------------|----------|----------|
| **Low (0â€“0.3)** | Deterministic, focused, consistent | Code generation, factual Q&A, math problems |
| **Medium (0.4â€“0.7)** | Balanced creativity and coherence | General conversation, content writing |
| **High (0.8â€“1.0)** | Creative, diverse, unpredictable | Brainstorming, storytelling, creative writing |

**Technical explanation:** Temperature adjusts the probability distribution over possible next tokens. Lower values make the model choose more likely tokens (conservative), while higher values flatten the distribution (exploratory).

### Choosing Bigger vs Smaller Models

| Aspect | Smaller Models (e.g., GPT-4o Mini) | Larger Models (e.g., GPT-4o) |
|--------|-----------------------------------|------------------------------|
| **Speed** | Fast (100-200ms) | Slower (500-1500ms) |
| **Cost** | Very cheap ($0.15/$0.60 per 1M tokens) | More expensive ($2.50/$10 per 1M tokens) |
| **Quality** | Good for straightforward tasks | Better reasoning, nuance, accuracy |
| **Context** | Smaller context window (16k-32k tokens) | Larger context window (128k+ tokens) |
| **Use Cases** | Chatbots, simple Q&A, classification | Complex analysis, coding, research |

**Rule of Thumb:** Start with the smallest model that meets your needs. Scale up only when you encounter limitations in reasoning, accuracy, or context length.

---

## ğŸ’­ Development Reflections

### Planning Approach
I began this project with physical notepad planning before writing any code. I sketched out the architecture, broke down tasks into time-boxed chunks, and estimated approximately 5 hours of work. This hands-on planning approach helped me visualize the component structure and data flow clearly. After completing the project, I formalized this planning into PLAN.md with both my original estimates and actual time spent. 

**Reflection:** While the notepad approach worked well for this individual task, for future projects I would create the formal PLAN.md document upfront, as it's more suitable for team collaboration and provides better documentation from the start.

### Task Completion
* **Estimated:** 5 hours (from notepad planning)
* **Actual:** 5 hours
* Task complexity was well-suited to the timeframe

### Development Approach
* **~90% self-written code** â€” Core architecture, business logic, and features
* **~10% assisted** â€” Some configuration boilerplate and styling patterns
* With heavy AI assistance: Could have built minimal version in ~30 minutes
* With moderate AI assistance: Could have built similar version in ~1.5-2 hours
* Chose manual approach to demonstrate problem-solving and architectural skills

### Hardest Part
No significant blockers, as I have experience with similar full-stack projects. The main challenges were:
- Balancing feature completeness with time constraints
- Deciding which "nice-to-have" features to include vs document for future

### Problem-Solving Approach
1. **Started with architecture** â€” Sketched component relationships and API structure on notepad
2. **Used TypeScript** â€” Type safety caught errors early, saved debugging time
3. **Built mock mode first** â€” Enabled rapid frontend development without API costs
4. **Incremental testing** â€” Tested each component as built rather than all at end

### What I Learned
- Balancing polish with time constraints
- Value of architectural decisions (services pattern made testing easier)

### What I Would Improve in Code/Structure

**If Given One More Day:**

#### High Priority (2-3 hours)
1. **Streaming Responses** 
   - Implement Server-Sent Events (SSE) for real-time chunk-by-chunk display
   - Better UX for longer responses (users see text appearing live)
   - Use OpenAI's streaming API with `stream: true`
   - Add abort controller to cancel in-flight requests
   - **Estimated:** 1.5 hours

2. **Database Integration**
   - PostgreSQL or MongoDB for persistent history
   - User authentication (JWT) for multi-user support
   - Cross-device history synchronization
   - Response caching to reduce API costs
   - **Estimated:** 2.5 hours

#### Medium Priority (1-2 hours)
3. **Enhanced UX Features**
   - Copy response to clipboard button
   - Export conversation history as JSON/PDF
   - Keyboard shortcuts (Ctrl+Enter to submit)
   - Dark mode toggle with theme persistence
   - Response regeneration button

4. **Production Readiness**
   - Rate limiting (prevent API abuse)
   - Request queuing for high traffic
   - Better error boundaries in React
   - Monitoring and logging (Winston/Pino)
   - Unit tests (Jest/Vitest)

#### Low Priority (1 hour)
5. **DevOps & Deployment**
   - Docker containerization
   - CI/CD pipeline (GitHub Actions)
   - Deploy frontend to Vercel
   - Deploy backend to Railway/Render
   - Environment-specific configs

**Current Strengths:**
- âœ… Clean, maintainable architecture
- âœ… Comprehensive error handling
- âœ… Professional API documentation
- âœ… Type-safe frontend
- âœ… Mock mode for cost-effective development

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/          # OpenAI client, Swagger setup
â”‚   â”‚   â”œâ”€â”€ controllers/     # Request handlers (prompt.controller.js)
â”‚   â”‚   â”œâ”€â”€ middlewares/     # Validation, error handling
â”‚   â”‚   â”œâ”€â”€ routes/          # API routes (prompt.routes.js)
â”‚   â”‚   â””â”€â”€ services/        # Business logic (llm.service.js)
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ server.js            # Entry point
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/             # Next.js pages (layout.tsx, page.tsx)
â”‚   â”‚   â”œâ”€â”€ components/      # React components (PromptForm, ResponseCard, etc.)
â”‚   â”‚   â”‚   â””â”€â”€ ui/          # Shadcn UI primitives
â”‚   â”‚   â””â”€â”€ lib/             # API client (api.ts), types (types.ts), utils
â”‚   â”œâ”€â”€ .env.local
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ next.config.js
â”‚
â”œâ”€â”€ PLAN.md                  # Time estimates and reflections
â””â”€â”€ README.md                # This file
```

---

## ğŸš€ API Endpoints

### `POST /api/generate`
Generate an AI response from a prompt.

**Request:**
```json
{
  "prompt": "Explain quantum computing",
  "model": "gpt-4o-mini",
  "temperature": 0.7
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "reply": "Quantum computing is...",
    "usedModel": "gpt-4o-mini", 
    "temperature": 0.7,
    "createdAt": "2024-01-15T10:30:00.000Z",
    "metadata": {
      "source": "mock",
      "responseTime": "150ms"
    }
  }
}
```

### `GET /api/models`
Get list of available AI models.

**Response:**
```json
{
  "success": true,
  "data": {
    "models": [
      {
        "id": "gpt-4o-mini",
        "name": "GPT-4o Mini",
        "description": "Fast and affordable - perfect for everyday tasks",
        "pricing": "Very affordable: $0.15/$0.60 per 1M tokens",
        "maxTokens": 16384,
        "recommended": true,
        "speed": "Fastest"
      },
      {
        "id": "gpt-4o",
        "name": "GPT-4o",
        "description": "Most capable model for complex reasoning",
        "pricing": "Premium: $2.50/$10.00 per 1M tokens",
        "maxTokens": 128000,
        "recommended": true,
        "speed": "Fast"
      },
      {
        "id": "gpt-4-turbo",
        "name": "GPT-4 Turbo",
        "description": "Previous generation flagship model with large context",
        "pricing": "Standard: $10.00/$30.00 per 1M tokens",
        "maxTokens": 128000,
        "recommended": false,
        "speed": "Moderate"
      }
    ]
  }
}
```

---

## ğŸ§ª Testing

**Test the backend:**
```bash
# Using curl
curl -X POST http://localhost:5000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Hello","model":"gpt-4o-mini","temperature":0.7}'

# Or visit Swagger docs at http://localhost:5000/api-docs
```


---

## ğŸ“ License

MIT

---

## ğŸ‘¤ Author

**Imran Bin Hasan**

*Built as a technical assessment for Core Devs Ltd.*
